{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib notebook\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from scipy.io import loadmat\n",
    "from scipy.fft import fft, fftshift\n",
    "import pywt\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.svm import SVC\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1st Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is SC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.980     0.840     0.905      1315\n",
      "         1.0      0.058     0.371     0.101        35\n",
      "\n",
      "    accuracy                          0.828      1350\n",
      "   macro avg      0.519     0.606     0.503      1350\n",
      "weighted avg      0.957     0.828     0.884      1350\n",
      "\n",
      "Test Output is\n",
      "GZVOU\n",
      "--------\n",
      "2 is SC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.988     0.818     0.895      1315\n",
      "         1.0      0.084     0.629     0.149        35\n",
      "\n",
      "    accuracy                          0.813      1350\n",
      "   macro avg      0.536     0.723     0.522      1350\n",
      "weighted avg      0.965     0.813     0.876      1350\n",
      "\n",
      "Test Output is\n",
      "LUKAS\n",
      "--------\n",
      "3 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.882     0.733     0.801       378\n",
      "         1.0      0.257     0.486     0.337        72\n",
      "\n",
      "    accuracy                          0.693       450\n",
      "   macro avg      0.570     0.609     0.569       450\n",
      "weighted avg      0.782     0.693     0.726       450\n",
      "\n",
      "Test Output is\n",
      "LUWT4\n",
      "--------\n",
      "4 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.928     0.792     0.855       375\n",
      "         1.0      0.400     0.693     0.507        75\n",
      "\n",
      "    accuracy                          0.776       450\n",
      "   macro avg      0.664     0.743     0.681       450\n",
      "weighted avg      0.840     0.776     0.797       450\n",
      "\n",
      "Test Output is\n",
      "LUKAS\n",
      "--------\n",
      "5 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.922     0.681     0.784       383\n",
      "         1.0      0.269     0.672     0.385        67\n",
      "\n",
      "    accuracy                          0.680       450\n",
      "   macro avg      0.596     0.677     0.584       450\n",
      "weighted avg      0.825     0.680     0.724       450\n",
      "\n",
      "Test Output is\n",
      "SGTJR\n",
      "--------\n",
      "6 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.895     0.715     0.795       368\n",
      "         1.0      0.327     0.622     0.429        82\n",
      "\n",
      "    accuracy                          0.698       450\n",
      "   macro avg      0.611     0.668     0.612       450\n",
      "weighted avg      0.791     0.698     0.728       450\n",
      "\n",
      "Test Output is\n",
      "WGXNJ\n",
      "--------\n",
      "7 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.894     0.688     0.777       381\n",
      "         1.0      0.242     0.551     0.336        69\n",
      "\n",
      "    accuracy                          0.667       450\n",
      "   macro avg      0.568     0.619     0.557       450\n",
      "weighted avg      0.794     0.667     0.710       450\n",
      "\n",
      "Test Output is\n",
      "2LZW8\n",
      "--------\n",
      "8 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.932     0.763     0.839       379\n",
      "         1.0      0.357     0.704     0.474        71\n",
      "\n",
      "    accuracy                          0.753       450\n",
      "   macro avg      0.645     0.733     0.656       450\n",
      "weighted avg      0.842     0.753     0.781       450\n",
      "\n",
      "Test Output is\n",
      "WATEL\n",
      "--------\n",
      "9 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.924     0.826     0.872       368\n",
      "         1.0      0.471     0.695     0.562        82\n",
      "\n",
      "    accuracy                          0.802       450\n",
      "   macro avg      0.698     0.761     0.717       450\n",
      "weighted avg      0.841     0.802     0.816       450\n",
      "\n",
      "Test Output is\n",
      "WATER\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Producing features for each person\n",
    "\n",
    "for k in range(1,10):\n",
    "    \n",
    "    # Train data for person k\n",
    "    train = pd.DataFrame(np.array(loadmat('TrainData' + str(k))['TrainData' + str(k)]))\n",
    "    \n",
    "    # Features matrix X_train\n",
    "    X_train = []\n",
    "    \n",
    "    # Finding the times when the 10th row is not zero\n",
    "    for i in np.argwhere(train.to_numpy()[9] != 0).reshape(-1, 4)[:,0]:\n",
    "        \n",
    "        # Producing features\n",
    "        x = train.iloc[1 : 9, i-28 : i+100].to_numpy()\n",
    "        x_train = np.append(np.array([]), x)\n",
    "        x_train = np.append(x_train, x.mean(axis=1))\n",
    "        x_train = np.append(x_train, x.var(axis=1))\n",
    "        x_train = np.append(x_train, np.corrcoef(x)[np.triu_indices(8, k=1)])\n",
    "        x_train = np.append(x_train, np.array(pywt.dwt(x, 'db1')))\n",
    "        x_train = np.append(x_train, abs(fftshift(fft(x)))[:, :64] ** 2)\n",
    "        X_train.append(x_train)\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    \n",
    "    # Train Labels y\n",
    "    y_train = train.iloc[10, np.argwhere(train.to_numpy()[9] != 0).reshape(-1, 4)[:,0]].to_numpy()\n",
    "    \n",
    "    \n",
    "    # Fisher-score\n",
    "    best_features = np.array([])\n",
    "    for i in range(len(X_train.T)):\n",
    "        \n",
    "        u0 = X_train.T[i].mean()\n",
    "        \n",
    "        n1 = (y_train == 0).sum()\n",
    "        u1 = X_train.T[i][y_train == 0].mean()\n",
    "        sigma2_1 = X_train.T[i][y_train == 0].var()\n",
    "        \n",
    "        n2 = (y_train == 1).sum()        \n",
    "        u2 = X_train.T[i][y_train == 1].mean()        \n",
    "        sigma2_2 = X_train.T[i][y_train == 1].var()\n",
    "        \n",
    "        best_features = np.append(best_features, (n1*(u1-u0)**2 + n2*(u2-u0)**2) / (n1*sigma2_1 + n2*sigma2_2))\n",
    "    best_features = np.argsort(best_features)    \n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Test data for person k\n",
    "    test = pd.DataFrame(np.array(loadmat('TestData' + str(k))['TestData' + str(k)]))\n",
    "    \n",
    "    # Features matrix X_test\n",
    "    X_test = []\n",
    "    \n",
    "    # Finding the times when the 10th row is not zero\n",
    "    for i in np.argwhere(test.to_numpy()[9] != 0).reshape(-1,4)[:,0]:        \n",
    "        row = np.array([])\n",
    "        \n",
    "        # Producing features\n",
    "        x = test.iloc[1 : 9, i-28 : i+100].to_numpy()\n",
    "        x_test = np.append(np.array([]), x)\n",
    "        x_test = np.append(x_test, x.mean(axis=1))\n",
    "        x_test = np.append(x_test, x.var(axis=1))\n",
    "        x_test = np.append(x_test, np.corrcoef(x)[np.triu_indices(8, k=1)])\n",
    "        x_test = np.append(x_test, np.array(pywt.dwt(x, 'db1')))\n",
    "        x_test = np.append(x_test, abs(fftshift(fft(x)))[:, :64] ** 2)\n",
    "        X_test.append(x_test)\n",
    "        \n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # The number which is shown in the 10th row\n",
    "    letter_codes = test.iloc[9,np.argwhere(test.to_numpy()[9] != 0).reshape(-1,4)[:,0]].to_numpy()    \n",
    "    \n",
    "    # Zeros to ones ratio: It is 35 for the aprroach \"Single Character\"\n",
    "    z21 = (y_train == 0).sum() // (y_train == 1).sum()\n",
    "    \n",
    "    if z21 == 35:\n",
    "        print(k,'is','SC')\n",
    "    else:\n",
    "        print(k,'is','RC')\n",
    "\n",
    "    \n",
    "    f1score = 0\n",
    "    # Max number of features will be 100\n",
    "    for i in range(1,100):\n",
    "        \n",
    "        x_train = X_train[:,best_features[-i:]]\n",
    "        x_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train,y_train, test_size=0.5, random_state=0)\n",
    "        \n",
    "        clf = LogisticRegression(solver='newton-cg', class_weight = {0:1,1:z21})\n",
    "        clf.fit(x_train_train,y_train_train)\n",
    "        \n",
    "        new_score = f1_score(y_train_test, clf.predict(x_train_test), average='weighted')\n",
    "        \n",
    "        if new_score > f1score:\n",
    "            num = i\n",
    "            f1score = new_score\n",
    "            report = classification_report(y_train_test, clf.predict(x_train_test), digits=3)\n",
    "            \n",
    "    print(report)\n",
    "    \n",
    "    X_train = X_train[:, best_features[-num:]]\n",
    "    X_test = X_test[:, best_features[-num:]]\n",
    "    \n",
    "    clf = LogisticRegression(solver='newton-cg', class_weight={0:1,1:z21})\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Test Output is')\n",
    "    \n",
    "    letters_lst = np.array(['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','0','1','2','3','4','5','6','7','8','9']).reshape((6,6))\n",
    "    \n",
    "    if z21 == 35:\n",
    "        for i in range(5):\n",
    "            code = np.unique(letter_codes[540*i : 540*(i+1)][clf.predict(X_test)[540*i : 540*(i+1)] == 1], return_counts=True)\n",
    "            print(letters_lst.flatten()[int(code[0][np.argsort(code[1])][-1])-1], end='')\n",
    "    else:\n",
    "        for i in range(5):\n",
    "            code = letter_codes[180*i : 180*(i+1)][clf.predict(X_test)[180*i : 180*(i+1)] == 1]\n",
    "            code1 = np.unique(code[code < 7], return_counts=True)\n",
    "            code2 = np.unique(code[code >= 7], return_counts=True)\n",
    "            row = int(code2[0][np.argsort(code2[1])][-1]) - 7\n",
    "            column = int(code1[0][np.argsort(code1[1])][-1])-1\n",
    "            print(letters_lst[row, column], end='')\n",
    "    print() \n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2nd Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 is SC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.985     0.802     0.884      1315\n",
      "         1.0      0.068     0.543     0.121        35\n",
      "\n",
      "    accuracy                          0.795      1350\n",
      "   macro avg      0.526     0.672     0.502      1350\n",
      "weighted avg      0.961     0.795     0.864      1350\n",
      "\n",
      "Test Output is\n",
      "7EV4Y\n",
      "--------\n",
      "2 is SC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.988     0.809     0.890      1315\n",
      "         1.0      0.081     0.629     0.143        35\n",
      "\n",
      "    accuracy                          0.804      1350\n",
      "   macro avg      0.534     0.719     0.516      1350\n",
      "weighted avg      0.964     0.804     0.870      1350\n",
      "\n",
      "Test Output is\n",
      "LUKAS\n",
      "--------\n",
      "3 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.880     0.717     0.790       378\n",
      "         1.0      0.246     0.486     0.327        72\n",
      "\n",
      "    accuracy                          0.680       450\n",
      "   macro avg      0.563     0.602     0.559       450\n",
      "weighted avg      0.779     0.680     0.716       450\n",
      "\n",
      "Test Output is\n",
      "FWEAY\n",
      "--------\n",
      "4 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.941     0.760     0.841       375\n",
      "         1.0      0.388     0.760     0.514        75\n",
      "\n",
      "    accuracy                          0.760       450\n",
      "   macro avg      0.664     0.760     0.677       450\n",
      "weighted avg      0.848     0.760     0.786       450\n",
      "\n",
      "Test Output is\n",
      "LUKAS\n",
      "--------\n",
      "5 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.925     0.640     0.756       383\n",
      "         1.0      0.254     0.701     0.373        67\n",
      "\n",
      "    accuracy                          0.649       450\n",
      "   macro avg      0.589     0.671     0.565       450\n",
      "weighted avg      0.825     0.649     0.699       450\n",
      "\n",
      "Test Output is\n",
      "WGTU3\n",
      "--------\n",
      "6 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.870     0.766     0.815       368\n",
      "         1.0      0.317     0.488     0.385        82\n",
      "\n",
      "    accuracy                          0.716       450\n",
      "   macro avg      0.594     0.627     0.600       450\n",
      "weighted avg      0.770     0.716     0.737       450\n",
      "\n",
      "Test Output is\n",
      "8M95T\n",
      "--------\n",
      "7 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.904     0.643     0.752       381\n",
      "         1.0      0.240     0.623     0.347        69\n",
      "\n",
      "    accuracy                          0.640       450\n",
      "   macro avg      0.572     0.633     0.549       450\n",
      "weighted avg      0.802     0.640     0.689       450\n",
      "\n",
      "Test Output is\n",
      "YXZC7\n",
      "--------\n",
      "8 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.917     0.731     0.814       379\n",
      "         1.0      0.311     0.648     0.420        71\n",
      "\n",
      "    accuracy                          0.718       450\n",
      "   macro avg      0.614     0.689     0.617       450\n",
      "weighted avg      0.822     0.718     0.751       450\n",
      "\n",
      "Test Output is\n",
      "WSZKR\n",
      "--------\n",
      "9 is RC\n",
      "              precision    recall  f1-score   support\n",
      "\n",
      "         0.0      0.918     0.826     0.870       368\n",
      "         1.0      0.462     0.671     0.547        82\n",
      "\n",
      "    accuracy                          0.798       450\n",
      "   macro avg      0.690     0.748     0.709       450\n",
      "weighted avg      0.835     0.798     0.811       450\n",
      "\n",
      "Test Output is\n",
      "WATER\n",
      "--------\n"
     ]
    }
   ],
   "source": [
    "\n",
    "### Producing features for each person\n",
    "\n",
    "for k in range(1,10):\n",
    "    \n",
    "    # Train data for person k\n",
    "    train = pd.DataFrame(np.array(loadmat('TrainData' + str(k))['TrainData' + str(k)]))\n",
    "    \n",
    "    # Features matrix X_train\n",
    "    X_train = []\n",
    "    \n",
    "    # Finding the times when the 10th row is not zero\n",
    "    for i in np.argwhere(train.to_numpy()[9] != 0).reshape(-1, 4)[:,0]:\n",
    "        \n",
    "        # Producing features\n",
    "        x = train.iloc[1 : 9, i-28 : i+100].to_numpy()\n",
    "        x_train = np.append(np.array([]), x)\n",
    "        x_train = np.append(x_train, x.mean(axis=1))\n",
    "        x_train = np.append(x_train, x.var(axis=1))\n",
    "        x_train = np.append(x_train, np.corrcoef(x)[np.triu_indices(8, k=1)])\n",
    "        x_train = np.append(x_train, np.array(pywt.dwt(x, 'db1')))\n",
    "        x_train = np.append(x_train, abs(fftshift(fft(x)))[:, :64] ** 2)\n",
    "        X_train.append(x_train)\n",
    "        \n",
    "    X_train = np.array(X_train)\n",
    "    \n",
    "    # Train Labels y\n",
    "    y_train = train.iloc[10, np.argwhere(train.to_numpy()[9] != 0).reshape(-1, 4)[:,0]].to_numpy()\n",
    "    \n",
    "    \n",
    "    # Fisher-score\n",
    "    best_features = np.array([])\n",
    "    for i in range(len(X_train.T)):\n",
    "        \n",
    "        u0 = X_train.T[i].mean()\n",
    "        \n",
    "        n1 = (y_train == 0).sum()\n",
    "        u1 = X_train.T[i][y_train == 0].mean()\n",
    "        sigma2_1 = X_train.T[i][y_train == 0].var()\n",
    "        \n",
    "        n2 = (y_train == 1).sum()        \n",
    "        u2 = X_train.T[i][y_train == 1].mean()        \n",
    "        sigma2_2 = X_train.T[i][y_train == 1].var()\n",
    "        \n",
    "        best_features = np.append(best_features, (n1*(u1-u0)**2 + n2*(u2-u0)**2) / (n1*sigma2_1 + n2*sigma2_2))\n",
    "    best_features = np.argsort(best_features)    \n",
    "    \n",
    "       \n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    # Test data for person k\n",
    "    test = pd.DataFrame(np.array(loadmat('TestData' + str(k))['TestData' + str(k)]))\n",
    "    \n",
    "    # Features matrix X_test\n",
    "    X_test = []\n",
    "    \n",
    "    # Finding the times when the 10th row is not zero\n",
    "    for i in np.argwhere(test.to_numpy()[9] != 0).reshape(-1,4)[:,0]:        \n",
    "        row = np.array([])\n",
    "        \n",
    "        # Producing features\n",
    "        x = test.iloc[1 : 9, i-28 : i+100].to_numpy()\n",
    "        x_test = np.append(np.array([]), x)\n",
    "        x_test = np.append(x_test, x.mean(axis=1))\n",
    "        x_test = np.append(x_test, x.var(axis=1))\n",
    "        x_test = np.append(x_test, np.corrcoef(x)[np.triu_indices(8, k=1)])\n",
    "        x_test = np.append(x_test, np.array(pywt.dwt(x, 'db1')))\n",
    "        x_test = np.append(x_test, abs(fftshift(fft(x)))[:, :64] ** 2)\n",
    "        X_test.append(x_test)\n",
    "        \n",
    "    X_test = np.array(X_test)\n",
    "    \n",
    "    scaler = preprocessing.StandardScaler().fit(X_train)\n",
    "    X_train = scaler.transform(X_train)\n",
    "    X_test = scaler.transform(X_test)\n",
    "    \n",
    "    # The number which is shown in the 10th row\n",
    "    letter_codes = test.iloc[9,np.argwhere(test.to_numpy()[9] != 0).reshape(-1,4)[:,0]].to_numpy()    \n",
    "    \n",
    "    # Zeros to ones ratio: It is 35 for the aprroach \"Single Character\"\n",
    "    z21 = (y_train == 0).sum() // (y_train == 1).sum()\n",
    "    \n",
    "    if z21 == 35:\n",
    "        print(k,'is','SC')\n",
    "    else:\n",
    "        print(k,'is','RC')\n",
    "\n",
    "    \n",
    "    f1score = 0\n",
    "    # Max number of features will be 100\n",
    "    for i in range(1,100):\n",
    "        \n",
    "        x_train = X_train[:,best_features[-i:]]\n",
    "        x_train_train, x_train_test, y_train_train, y_train_test = train_test_split(x_train,y_train, test_size=0.5, random_state=0)\n",
    "        \n",
    "        clf = clf = svm.SVC(C=0.1,kernel='linear',class_weight={0:1,1:z21})\n",
    "        clf.fit(x_train_train,y_train_train)\n",
    "        \n",
    "        new_score = f1_score(y_train_test, clf.predict(x_train_test), average='weighted')\n",
    "        \n",
    "        if new_score > f1score:\n",
    "            num = i\n",
    "            f1score = new_score\n",
    "            report = classification_report(y_train_test, clf.predict(x_train_test), digits=3)\n",
    "            \n",
    "    print(report)\n",
    "    \n",
    "    X_train = X_train[:, best_features[-num:]]\n",
    "    X_test = X_test[:, best_features[-num:]]\n",
    "    \n",
    "    clf = svm.SVC(C=0.1,kernel='linear',class_weight={0:1,1:z21})\n",
    "    clf.fit(X_train, y_train)\n",
    "    \n",
    "    print('Test Output is')\n",
    "    \n",
    "    letters_lst = np.array(['A','B','C','D','E','F','G','H','I','J','K','L','M','N','O','P','Q','R','S','T','U','V','W','X','Y','Z','0','1','2','3','4','5','6','7','8','9']).reshape((6,6))\n",
    "    \n",
    "    if z21 == 35:\n",
    "        for i in range(5):\n",
    "            code = np.unique(letter_codes[540*i : 540*(i+1)][clf.predict(X_test)[540*i : 540*(i+1)] == 1], return_counts=True)\n",
    "            print(letters_lst.flatten()[int(code[0][np.argsort(code[1])][-1])-1], end='')\n",
    "    else:\n",
    "        for i in range(5):\n",
    "            code = letter_codes[180*i : 180*(i+1)][clf.predict(X_test)[180*i : 180*(i+1)] == 1]\n",
    "            code1 = np.unique(code[code < 7], return_counts=True)\n",
    "            code2 = np.unique(code[code >= 7], return_counts=True)\n",
    "            row = int(code2[0][np.argsort(code2[1])][-1]) - 7\n",
    "            column = int(code1[0][np.argsort(code1[1])][-1])-1\n",
    "            print(letters_lst[row, column], end='')\n",
    "    print() \n",
    "    print('--------')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
